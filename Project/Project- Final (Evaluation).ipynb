{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bbcec44",
   "metadata": {},
   "source": [
    "# Traffic Sign Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e10bff",
   "metadata": {},
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc285e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Setup\n",
    "import sys\n",
    "# Python 3.7 is required\n",
    "assert sys.version_info >= (3,7)\n",
    "\n",
    "# Import Library\n",
    "import os\n",
    "import time\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Make sure that optimization is enabled\n",
    "if not cv.useOptimized():\n",
    "    cv.setUseOptimized(True)\n",
    "\n",
    "cv.useOptimized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23d6f9",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd166112",
   "metadata": {},
   "source": [
    "### 1) Image Preprocessing\n",
    "- **Gaussian Filtering**  \n",
    " 1) To remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361a4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_preprocessing (image):\n",
    "    # Variable\n",
    "    kernel_size = 7\n",
    "    \n",
    "    # Make a copy image\n",
    "    img_copy = image.copy()\n",
    "    \n",
    "    # Gaussian filter\n",
    "    img_gaussian = cv.GaussianBlur(img_copy, (kernel_size,kernel_size), 1)\n",
    "    \n",
    "    return img_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ddab1",
   "metadata": {},
   "source": [
    "### 2) Largest Contour Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0fc0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_contour_detect(images):\n",
    "    # Find contours\n",
    "    _, image = cv.threshold(images, 200, 255, cv.THRESH_TOZERO)\n",
    "    contours, hierarchy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Contour List\n",
    "    if (len(contours) == 0):\n",
    "        white_mask = np.zeros(image.shape, dtype = np.uint8)\n",
    "        return white_mask, white_mask, white_mask\n",
    "    else:\n",
    "        cnt_list = np.zeros(len(contours))\n",
    "        for i in range(0,len(contours)):\n",
    "            cnt_list[i] = cv.contourArea(contours[i])\n",
    "        \n",
    "    # Find largest contour\n",
    "    largest_contour_index = np.argmax(cnt_list)\n",
    "    largest_contour = contours[largest_contour_index]\n",
    "    contour_mask = np.zeros(image.shape, dtype = np.uint8)\n",
    "    \n",
    "    # Draw Contour\n",
    "    if len(contours) != 0:\n",
    "        cv.drawContours(contour_mask, contours, largest_contour_index, (255,255,255), -1)\n",
    "        \n",
    "    return image, largest_contour, contour_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b02fe2",
   "metadata": {},
   "source": [
    "### 3) Bounding Box Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00fc8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box(image, largest_contour):\n",
    "    # Make a copy image\n",
    "    bounding_box = image.copy()\n",
    "    \n",
    "    # If cannot find the contour\n",
    "    if (largest_contour is None):\n",
    "        return bounding_box, (0,0,0,0)\n",
    "    \n",
    "    x,y,w,h = cv.boundingRect(largest_contour)\n",
    "    cv.rectangle(bounding_box,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "    \n",
    "    return bounding_box, (x,y,x+w,y+h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1620aa",
   "metadata": {},
   "source": [
    "### 4) Image Segmentation\n",
    "- **HSV Color segmentation**: To segment out blue color\n",
    "\n",
    "- **Canny Edge Detection**: To detect the edge of traffic sign in mask\n",
    "\n",
    "- **Morphological Prpcessing**\n",
    "    - **Dilation** : To expand inside white region\n",
    "    - **Erosion** : To decrease outside white region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9982b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_segmentation(annotation_data, image, traffic_sign, save_res):\n",
    "    # Variable\n",
    "    sec = time.time()\n",
    "    traffic_signs = np.where(annotation_data['Category'].isin(traffic_sign))[0]\n",
    "    all = np.empty((150,1200,3))\n",
    "    points = []\n",
    "    accuracy = [0,0,0,0,0]\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    # set Blue range\n",
    "    blue_low, blue_high = (90, 60, 10), (135, 255, 255)\n",
    "    \n",
    "    for i in range (len(traffic_signs)):\n",
    "        # Copy each image\n",
    "        image_copy = image[traffic_signs[i]].copy()\n",
    "        h, w = image_copy.shape[:2]\n",
    "        \n",
    "        ## Image Preprocessing\n",
    "        img= img_preprocessing(image_copy)\n",
    "    \n",
    "        # HSV color segmentation\n",
    "        img_hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    \n",
    "        # Blue mask\n",
    "        hsv_mask = cv.inRange(img_hsv, blue_low, blue_high)\n",
    "    \n",
    "        # Canny edge detection\n",
    "        img_canny = cv.Canny(hsv_mask, 50, 100)\n",
    "    \n",
    "        # Morphological Processing\n",
    "        img_Dil = cv.dilate(img_canny, kernel, iterations = 1)\n",
    "        img_Ero = cv.erode(img_Dil, kernel, iterations = 1)\n",
    "    \n",
    "        # Find largest contour detection\n",
    "        th, largest_contour, contour_mask = largest_contour_detect(img_Ero)\n",
    "    \n",
    "        # Bounding box detection\n",
    "        img_bounding_box, box_point = bounding_box(img, largest_contour)\n",
    "    \n",
    "        points.append(box_point)\n",
    "    \n",
    "        # calculating IOU accuracy and storing into different category accuracy group\n",
    "        area_1 = (box_point[2] - box_point[0]) * (box_point[3] - box_point[1])\n",
    "        area_2 = (train_annotation['End_X'][i] - train_annotation['Start_X'][i]) * (train_annotation['End_Y'][i] - train_annotation['Start_Y'][i])\n",
    "        diff_X = max(0, min(box_point[2], train_annotation['End_X'][i]) - max(box_point[0], train_annotation['Start_X'][i]))\n",
    "        diff_Y = max(0, min(box_point[3], train_annotation['End_Y'][i]) - max(box_point[1], train_annotation['Start_Y'][i]))\n",
    "        overlap = diff_X * diff_Y\n",
    "        final_area = area_1 + area_2 - overlap\n",
    "        accry = round(overlap/final_area*100,2)\n",
    "        \n",
    "        if accry >= 95:\n",
    "            accuracy[0] += 1\n",
    "        elif accry >= 90:\n",
    "            accuracy[1] += 1\n",
    "        elif accry >= 85:\n",
    "            accuracy[2] += 1\n",
    "        elif accry >= 80:\n",
    "            accuracy[3] += 1\n",
    "        else:\n",
    "            accuracy[4] += 1\n",
    "    \n",
    "        if(save_res):\n",
    "            hsv_mask = np.stack((hsv_mask,)*3, axis=-1)\n",
    "            img_canny = np.stack((img_canny,)*3, axis=-1)\n",
    "            img_Dil = np.stack((img_Dil,)*3, axis=-1)\n",
    "            img_Ero = np.stack((img_Ero,)*3, axis=-1)\n",
    "            final_mask = np.stack((contour_mask,)*3, axis=-1)\n",
    "            result = np.concatenate((image_copy, hsv_mask, img_canny, img_Dil, img_Ero, final_mask, img_bounding_box), axis = 1)\n",
    "            result = cv.resize(result, (1050,150), interpolation = cv.INTER_CUBIC)\n",
    "            result = np.concatenate((result, np.zeros((150, 150, 3))), axis = 1)\n",
    "            result = np.concatenate((np.zeros((30, 1200, 3)), result), axis = 0)\n",
    "        \n",
    "            cv.putText(result, (\"Original Image\"), (5, 15), 0, 0.5, (255,255,255), 1, cv.INTER_CUBIC)\n",
    "            cv.putText(result, (\"HSV Mask\"), (155, 15), 0, 0.5, (255,255,255), 1, cv.INTER_CUBIC)\n",
    "            cv.putText(result, (\"Canny Edge\"), (305, 15), 0, 0.5, (255,255,255), 1, cv.INTER_CUBIC)\n",
    "            cv.putText(result, (\"Dilation\"), (455, 15), 0, 0.5, (255,255,255), 1, cv.INTER_CUBIC)\n",
    "            cv.putText(result, (\"Erosion\"), (605, 15), 0, 0.5, (255,255,255), 1, cv.INTER_CUBIC)\n",
    "            cv.putText(result, (\"Contour\"), (755, 15), 0, 0.5, (255,255,255), 1, cv.INTER_CUBIC)\n",
    "            cv.putText(result, (\"Bounding Box\"), (905, 15), 0, 0.5, (255,255,255), 1, cv.INTER_CUBIC)\n",
    "            cv.putText(result, (\"Accuracy\"), (1055, 15), 0, 0.5, (255,255,255), 1, cv.INTER_CUBIC)\n",
    "            cv.putText(result, str(accry) + \"%\", (1100, 110), 0, 0.5, (255,255,255), 1, cv.INTER_CUBIC)\n",
    "                \n",
    "            all = np.concatenate((all, result), axis = 0)\n",
    "    \n",
    "            if (i%100 == 0):\n",
    "                cv.imwrite('result' + str(i//100) + '.png', all)\n",
    "                all = result\n",
    "                print(i, \"/\", len(traffic_signs), \"Elapsed time:\", time.time() - sec, \"Average time:\", (time.time() - sec)/(i+1))\n",
    "            elif (i%100 == 0):\n",
    "                print(i, \"/\", len(traffic_signs), \"Elapsed time:\", time.time() - sec, \"Average time:\", (time.time() - sec)/(i+1))\n",
    "    \n",
    "    return points, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77027b3",
   "metadata": {},
   "source": [
    "## Load Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a720f1bd",
   "metadata": {},
   "source": [
    "**1) Load TSRD Train Annotation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c98609cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the TSRD train annotation txt file\n",
    "path = os.path.join(os.getcwd(), 'TSRD-Train Annotation\\\\TsignRecgTrain4170Annotation.txt')\n",
    "\n",
    "# Column's names\n",
    "columns = ['File Name', 'Width', 'Height', 'Start_X', 'Start_Y', 'End_X', 'End_Y', 'Category']\n",
    "\n",
    "# Read the data of the TSRD train annotation txt file to the train_annotation\n",
    "train_annotation = pd.read_csv(path, sep=\";\", index_col = False, names = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b0302a",
   "metadata": {},
   "source": [
    "**2) Load Train Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b078dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the folder containing the trains set image files\n",
    "path = os.path.join(os.getcwd(), 'TSRD-train')\n",
    "\n",
    "#read the train images into train_image\n",
    "train_image = []\n",
    "for i in range (train_annotation.shape[0]):\n",
    "    train_image.append(cv.imread(path + '\\\\' + train_annotation.iloc[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124c9597",
   "metadata": {},
   "source": [
    "## Segmentation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ae7ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4170 Elapsed time: 0.02698373794555664 Average time: 0.02698373794555664\n",
      "100 / 4170 Elapsed time: 12.092069864273071 Average time: 0.11972346400270367\n",
      "200 / 4170 Elapsed time: 22.895881414413452 Average time: 0.1139098577831515\n",
      "300 / 4170 Elapsed time: 33.6307315826416 Average time: 0.11173000525794552\n",
      "400 / 4170 Elapsed time: 44.58791995048523 Average time: 0.11119182032539958\n",
      "500 / 4170 Elapsed time: 55.33595156669617 Average time: 0.11045100113113007\n",
      "600 / 4170 Elapsed time: 66.17390203475952 Average time: 0.11010632618096426\n",
      "700 / 4170 Elapsed time: 76.96869969367981 Average time: 0.10979843037614809\n",
      "800 / 4170 Elapsed time: 87.99837946891785 Average time: 0.10986064852549045\n",
      "900 / 4170 Elapsed time: 98.70924496650696 Average time: 0.10955521083963037\n",
      "1000 / 4170 Elapsed time: 109.348149061203 Average time: 0.10923891015105196\n",
      "1100 / 4170 Elapsed time: 120.18194270133972 Average time: 0.10915707783954562\n",
      "1200 / 4170 Elapsed time: 130.86782121658325 Average time: 0.10896571291971961\n",
      "1300 / 4170 Elapsed time: 141.39379167556763 Average time: 0.10868085447776143\n",
      "1400 / 4170 Elapsed time: 152.06301593780518 Average time: 0.10853891216117428\n",
      "1500 / 4170 Elapsed time: 162.75888919830322 Average time: 0.1084336370408416\n",
      "1600 / 4170 Elapsed time: 173.39879417419434 Average time: 0.10830655476214512\n",
      "1700 / 4170 Elapsed time: 184.0506935119629 Average time: 0.1082014659094432\n",
      "1800 / 4170 Elapsed time: 194.63179302215576 Average time: 0.10806873571468949\n",
      "1900 / 4170 Elapsed time: 205.19975066184998 Average time: 0.10794305663432403\n",
      "2000 / 4170 Elapsed time: 215.75170636177063 Average time: 0.10782194220978042\n",
      "2100 / 4170 Elapsed time: 226.34164023399353 Average time: 0.1077304332384548\n",
      "2200 / 4170 Elapsed time: 237.3025279045105 Average time: 0.10781577823921422\n",
      "2300 / 4170 Elapsed time: 247.90645122528076 Average time: 0.10773857071937451\n",
      "2400 / 4170 Elapsed time: 258.5488302707672 Average time: 0.10768381102489263\n",
      "2500 / 4170 Elapsed time: 269.04381799697876 Average time: 0.10757449739983158\n",
      "2600 / 4170 Elapsed time: 279.57178711891174 Average time: 0.10748626955744396\n",
      "2700 / 4170 Elapsed time: 290.28365087509155 Average time: 0.10747265859870106\n",
      "2800 / 4170 Elapsed time: 300.90356731414795 Average time: 0.10742719290044554\n",
      "2900 / 4170 Elapsed time: 311.3215899467468 Average time: 0.10731526713090205\n",
      "3000 / 4170 Elapsed time: 321.81978607177734 Average time: 0.10723751618519738\n",
      "3100 / 4170 Elapsed time: 332.29678297042847 Average time: 0.1071579435570553\n",
      "3200 / 4170 Elapsed time: 342.9446830749512 Average time: 0.10713673323178731\n",
      "3300 / 4170 Elapsed time: 353.563600063324 Average time: 0.10710802788952559\n",
      "3400 / 4170 Elapsed time: 364.239714384079 Average time: 0.10709782839872949\n",
      "3500 / 4170 Elapsed time: 374.8360471725464 Average time: 0.10706542335691129\n",
      "3600 / 4170 Elapsed time: 385.3222327232361 Average time: 0.10700423013697198\n",
      "3700 / 4170 Elapsed time: 395.89427399635315 Average time: 0.10696954174448883\n",
      "3800 / 4170 Elapsed time: 406.6072931289673 Average time: 0.1069737682528196\n",
      "3900 / 4170 Elapsed time: 417.21950483322144 Average time: 0.10695193664014904\n",
      "4000 / 4170 Elapsed time: 427.60055804252625 Average time: 0.10687342115534272\n",
      "4100 / 4170 Elapsed time: 438.0805549621582 Average time: 0.10682286148796835\n"
     ]
    }
   ],
   "source": [
    "# All traffic signs values\n",
    "ts_cat = tuple(range(0, 58))\n",
    "\n",
    "points, accuracy = img_segmentation(train_annotation, train_image, ts_cat, save_res = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f19958",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6fd8d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 4170\n",
      "Overall Accuracy of Segmentation: 0.3030884739095587\n",
      "Number of 95% - 100% Accuracy: 574\n",
      "Number of 90% - 95% Accuracy: 270\n",
      "Number of 85% - 90% Accuracy: 52\n",
      "Number of 80% - 85% Accuracy: 50\n",
      "Number of lower than 80% Accuracy: 3224\n"
     ]
    }
   ],
   "source": [
    "## Evaluation result\n",
    "overlap_total = 0\n",
    "final_area = 0\n",
    "\n",
    "for i in range (len(points)):\n",
    "    area_1 = (points[i][2] - points[i][0]) * (points[i][3] - points[i][1])\n",
    "    area_2 = (train_annotation['End_X'][i] - train_annotation['Start_X'][i]) * (train_annotation['End_Y'][i] - train_annotation['Start_Y'][i])\n",
    "    diff_X = max(0, min(points[i][2], train_annotation['End_X'][i]) - max(points[i][0], train_annotation['Start_X'][i]))\n",
    "    diff_Y = max(0, min(points[i][3], train_annotation['End_Y'][i]) - max(points[i][1], train_annotation['Start_Y'][i]))\n",
    "    overlap = diff_X * diff_Y\n",
    "    overlap_total += overlap\n",
    "    final_area = final_area + area_1 + area_2 - overlap\n",
    "\n",
    "print(\"Total Images:\", str(accuracy[0] + accuracy[1] + accuracy[2] + accuracy[3] + accuracy[4]))\n",
    "print(\"Overall Accuracy of Segmentation:\", overlap_total/final_area)\n",
    "print(\"Number of 95% - 100% Accuracy:\", str(accuracy[0]))\n",
    "print(\"Number of 90% - 95% Accuracy:\", str(accuracy[1]))\n",
    "print(\"Number of 85% - 90% Accuracy:\", str(accuracy[2]))\n",
    "print(\"Number of 80% - 85% Accuracy:\", str(accuracy[3]))\n",
    "print(\"Number of lower than 80% Accuracy:\", str(accuracy[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f21a4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The total overall is quite not good, this is because we only focus on blue color traffic signs. However, we have successfully segment more than 946 blue color traffic sign images and it is achieved our objective which is more than 70 images had 80 percent accuracy. In addition, we have 574 images is achieved more than 95 percent accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4284d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
